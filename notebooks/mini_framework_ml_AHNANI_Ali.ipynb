{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": "# Mini Framework ML From Scratch\n## Projet M2 — AHNANI Ali\n### Python + NumPy uniquement — Autodiff · Optimizers · MLP · Kernel · MNIST\n\n---\n> **Objectif** : Construire un pipeline ML complet sans sklearn/pytorch/tensorflow.  \n> Toutes les briques sont implémentées from scratch : autodiff, optimizers, modèles, dataset réel.\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-0",
   "metadata": {},
   "source": "## 0. Imports & Configuration globale"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport struct, gzip, os, time, warnings\nfrom copy import deepcopy\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\nplt.rcParams.update({\n    'figure.dpi': 120,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False,\n})\n\nprint(\"NumPy version :\", np.__version__)\nprint(\"Projet : Mini Framework ML From Scratch\")\nprint(\"Auteur : AHNANI Ali\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-1",
   "metadata": {},
   "source": "---\n## PARTIE 1 — Moteur Autodiff (from scratch)\n\n### Principe\nOn construit un moteur de différentiation automatique en **mode reverse** (backpropagation).  \nChaque `Tensor` stocke :\n- sa valeur (`data`)\n- son gradient accumulé (`grad`)\n- les opérations parents (graphe dynamique)\n- une fonction backward locale\n\n### Complexité\n- **Temps forward** : O(|ops|) — proportionnel au nombre d'opérations\n- **Temps backward** : O(|graph|) — tri topologique + backward de chaque nœud\n- **Mémoire** : O(|graph|) — on stocke les activations intermédiaires\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1a",
   "metadata": {},
   "outputs": [],
   "source": "class Tensor:\n    \"\"\"\n    Tensor avec support de la differentiation automatique (reverse-mode autodiff).\n    Implémente un graphe computationnel dynamique (define-by-run).\n    \"\"\"\n    def __init__(self, data, requires_grad=False, _children=(), _op=''):\n        self.data = np.array(data, dtype=np.float64)\n        self.grad = np.zeros_like(self.data)\n        self.requires_grad = requires_grad\n        self._backward = lambda: None\n        self._prev = set(_children)\n        self._op = _op\n\n    # ----------- Opérations arithmétiques de base -----------\n\n    def __add__(self, other):\n        other = other if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data + other.data, _children=(self, other), _op='add')\n\n        def _backward():\n            # Accumulation + gestion du broadcast\n            grad = out.grad\n            self.grad += _unbroadcast(grad, self.data.shape)\n            other.grad += _unbroadcast(grad, other.data.shape)\n        out._backward = _backward\n        return out\n\n    def __mul__(self, other):\n        other = other if isinstance(other, Tensor) else Tensor(other)\n        out = Tensor(self.data * other.data, _children=(self, other), _op='mul')\n\n        def _backward():\n            self.grad += _unbroadcast(other.data * out.grad, self.data.shape)\n            other.grad += _unbroadcast(self.data * out.grad, other.data.shape)\n        out._backward = _backward\n        return out\n\n    def __matmul__(self, other):\n        out = Tensor(self.data @ other.data, _children=(self, other), _op='matmul')\n\n        def _backward():\n            self.grad += out.grad @ other.data.T\n            other.grad += self.data.T @ out.grad\n        out._backward = _backward\n        return out\n\n    def __neg__(self):\n        return self * -1\n\n    def __sub__(self, other):\n        return self + (-other)\n\n    def __radd__(self, other): return self + other\n    def __rmul__(self, other): return self * other\n    def __rsub__(self, other): return other + (-self)\n    def __truediv__(self, other): return self * other**-1\n\n    def __pow__(self, exp):\n        assert isinstance(exp, (int, float))\n        out = Tensor(self.data**exp, _children=(self,), _op=f'pow{exp}')\n\n        def _backward():\n            self.grad += exp * (self.data**(exp-1)) * out.grad\n        out._backward = _backward\n        return out\n\n    # ----------- Opérations matricielles -----------\n\n    def T(self):\n        out = Tensor(self.data.T, _children=(self,), _op='transpose')\n        def _backward():\n            self.grad += out.grad.T\n        out._backward = _backward\n        return out\n\n    def reshape(self, *shape):\n        original_shape = self.data.shape\n        out = Tensor(self.data.reshape(*shape), _children=(self,), _op='reshape')\n        def _backward():\n            self.grad += out.grad.reshape(original_shape)\n        out._backward = _backward\n        return out\n\n    # ----------- Opérations d'activation -----------\n\n    def exp(self):\n        e = np.exp(np.clip(self.data, -500, 500))\n        out = Tensor(e, _children=(self,), _op='exp')\n        def _backward():\n            self.grad += e * out.grad\n        out._backward = _backward\n        return out\n\n    def log(self):\n        out = Tensor(np.log(self.data + 1e-15), _children=(self,), _op='log')\n        def _backward():\n            self.grad += (1.0 / (self.data + 1e-15)) * out.grad\n        out._backward = _backward\n        return out\n\n    def relu(self):\n        out = Tensor(np.maximum(0, self.data), _children=(self,), _op='relu')\n        def _backward():\n            self.grad += (self.data > 0).astype(float) * out.grad\n        out._backward = _backward\n        return out\n\n    def gelu(self):\n        # GELU approx: x * sigmoid(1.702 * x)\n        sig = 1.0 / (1.0 + np.exp(-1.702 * self.data))\n        out = Tensor(self.data * sig, _children=(self,), _op='gelu')\n        def _backward():\n            dsig = sig * (1 - sig)\n            dgelu = sig + self.data * 1.702 * dsig\n            self.grad += dgelu * out.grad\n        out._backward = _backward\n        return out\n\n    # ----------- Réductions -----------\n\n    def sum(self, axis=None, keepdims=False):\n        out = Tensor(self.data.sum(axis=axis, keepdims=keepdims),\n                     _children=(self,), _op='sum')\n        def _backward():\n            grad = out.grad\n            if axis is not None and not keepdims:\n                grad = np.expand_dims(grad, axis=axis)\n            self.grad += np.broadcast_to(grad, self.data.shape)\n        out._backward = _backward\n        return out\n\n    def mean(self, axis=None, keepdims=False):\n        n = self.data.size if axis is None else self.data.shape[axis]\n        return self.sum(axis=axis, keepdims=keepdims) * (1.0 / n)\n\n    # ----------- Backward (algorithme principal) -----------\n\n    def backward(self):\n        \"\"\"Lance la backpropagation depuis ce tenseur (scalar).\"\"\"\n        topo = []\n        visited = set()\n\n        def build_topo(v):\n            if id(v) not in visited:\n                visited.add(id(v))\n                for child in v._prev:\n                    build_topo(child)\n                topo.append(v)\n\n        build_topo(self)\n        self.grad = np.ones_like(self.data)\n\n        for node in reversed(topo):\n            node._backward()\n\n    def zero_grad(self):\n        self.grad = np.zeros_like(self.data)\n\n    def __repr__(self):\n        return f\"Tensor(shape={self.data.shape}, op='{self._op}')\"\n\n\ndef _unbroadcast(grad, shape):\n    \"\"\"Réduit le gradient pour correspondre à la forme d'origine (gestion broadcast).\"\"\"\n    while len(grad.shape) > len(shape):\n        grad = grad.sum(axis=0)\n    for i, (gs, ss) in enumerate(zip(grad.shape, shape)):\n        if ss == 1:\n            grad = grad.sum(axis=i, keepdims=True)\n    return grad\n\n\nprint(\"✓ Classe Tensor implémentée avec autodiff complet\")\nprint(\"  Opérations supportées: add, mul, matmul, pow, exp, log, relu, gelu, sum, mean, transpose, reshape\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1b",
   "metadata": {},
   "outputs": [],
   "source": "# ─── TESTS AUTODIFF ───────────────────────────────────────────────\nprint(\"=\" * 60)\nprint(\"TESTS AUTODIFF\")\nprint(\"=\" * 60)\n\n# Test 1 : gradient scalaire\nx = Tensor(3.0)\ny = x * x + x * 2 + Tensor(1.0)   # y = x^2 + 2x + 1 => dy/dx = 2x+2 = 8\ny.backward()\nprint(f\"Test 1 — y=x²+2x+1, x=3 | grad_x={x.grad:.4f} (attendu: 8.0)\")\n\n# Test 2 : matmul gradient\nA = Tensor(np.random.randn(3, 4))\nB = Tensor(np.random.randn(4, 5))\nC = A @ B\nloss = C.mean()\nloss.backward()\nprint(f\"Test 2 — matmul(3,4)@(4,5) | grad_A shape={A.grad.shape} ✓\")\n\n# Test 3 : chaîne exp/log\nx = Tensor(np.array([1.0, 2.0, 3.0]))\ny = (x.exp()).log()   # log(exp(x)) = x => grad = 1\ny.mean().backward()\nprint(f\"Test 3 — log(exp(x)) | grad={x.grad.round(4)} (attendu: [0.333, 0.333, 0.333])\")\n\n# Test 4 : broadcast dans add\na = Tensor(np.ones((3, 4)))\nb = Tensor(np.ones((1, 4)))\nc = (a + b).sum()\nc.backward()\nprint(f\"Test 4 — broadcast add | grad_b={b.grad} (attendu: [[3,3,3,3]])\")\n\n# Test 5 : gradient ReLU\nx = Tensor(np.array([-2.0, 0.0, 3.0]))\ny = x.relu().sum()\ny.backward()\nprint(f\"Test 5 — ReLU grad | grad={x.grad} (attendu: [0, 0, 1])\")\n\nprint()\nprint(\"✓ Tous les tests autodiff passent\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1c",
   "metadata": {},
   "outputs": [],
   "source": "# ─── VISUALISATION GRAPHE COMPUTATIONNEL ────────────────────────\ndef count_graph_nodes(tensor):\n    \"\"\"Compte les noeuds du graphe computationnel.\"\"\"\n    visited = set()\n    count = [0]\n    def dfs(v):\n        if id(v) not in visited:\n            visited.add(id(v))\n            count[0] += 1\n            for child in v._prev:\n                dfs(child)\n    dfs(tensor)\n    return count[0]\n\n# Exemple : petit réseau\nnp.random.seed(0)\nx = Tensor(np.random.randn(5, 3))\nW = Tensor(np.random.randn(3, 4))\nb = Tensor(np.zeros((1, 4)))\nh = (x @ W + b).relu()\nW2 = Tensor(np.random.randn(4, 2))\nb2 = Tensor(np.zeros((1, 2)))\nout = h @ W2 + b2\nloss = out.mean()\n\nn_nodes = count_graph_nodes(loss)\nprint(f\"Graphe computationnel : {n_nodes} noeuds pour un mini MLP (5x3 -> 4 -> 2)\")\n\n# Visualiser la taille du graphe en fonction de la profondeur\ndepths = [1, 2, 3, 4, 5, 6, 8, 10]\nsizes = []\nfor d in depths:\n    x_ = Tensor(np.random.randn(10, 8))\n    layer = x_\n    for _ in range(d):\n        W_ = Tensor(np.random.randn(8, 8))\n        layer = (layer @ W_).relu()\n    sizes.append(count_graph_nodes(layer))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(depths, sizes, 'o-', color='steelblue', lw=2, ms=7)\nax.set_xlabel(\"Profondeur réseau (nb de couches)\")\nax.set_ylabel(\"Noeuds dans le graphe\")\nax.set_title(\"Complexité mémoire du graphe computationnel — O(|graph|)\")\nax.fill_between(depths, sizes, alpha=0.15, color='steelblue')\nplt.tight_layout()\nplt.savefig('/home/claude/fig_graph_complexity.png', dpi=120, bbox_inches='tight')\nplt.show()\nprint(\"Figure sauvegardée : fig_graph_complexity.png\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-2",
   "metadata": {},
   "source": "---\n## PARTIE 2 — Optimiseurs (from scratch)\n\nOn implémente quatre optimiseurs classiques :\n\n| Optimiseur | Mise à jour | Avantage |\n|-----------|------------|----------|\n| SGD | θ ← θ − η∇L | Simple, rapide |\n| Momentum | v ← βv + ∇L, θ ← θ − ηv | Accélère convergence |\n| RMSProp | v ← ρv + (1-ρ)∇²L, θ ← θ − η/√v · ∇L | Adaptatif |\n| Adam | m, v adaptatifs, biais corrigé | Meilleur en pratique |\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2a",
   "metadata": {},
   "outputs": [],
   "source": "class Optimizer:\n    \"\"\"Classe de base pour les optimiseurs.\"\"\"\n    def __init__(self, params, lr=0.01):\n        self.params = params\n        self.lr = lr\n\n    def zero_grad(self):\n        for p in self.params:\n            p.grad = np.zeros_like(p.data)\n\n    def step(self):\n        raise NotImplementedError\n\n\nclass SGD(Optimizer):\n    \"\"\"Stochastic Gradient Descent avec Momentum optionnel.\"\"\"\n    def __init__(self, params, lr=0.01, momentum=0.0, weight_decay=0.0):\n        super().__init__(params, lr)\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.velocities = [np.zeros_like(p.data) for p in params]\n\n    def step(self):\n        for i, p in enumerate(self.params):\n            g = p.grad + self.weight_decay * p.data\n            if self.momentum > 0:\n                self.velocities[i] = self.momentum * self.velocities[i] + g\n                p.data -= self.lr * self.velocities[i]\n            else:\n                p.data -= self.lr * g\n\n\nclass Momentum(Optimizer):\n    \"\"\"SGD avec momentum classique (Polyak).\"\"\"\n    def __init__(self, params, lr=0.01, beta=0.9):\n        super().__init__(params, lr)\n        self.beta = beta\n        self.v = [np.zeros_like(p.data) for p in params]\n\n    def step(self):\n        for i, p in enumerate(self.params):\n            self.v[i] = self.beta * self.v[i] + p.grad\n            p.data -= self.lr * self.v[i]\n\n\nclass RMSProp(Optimizer):\n    \"\"\"RMSProp — normalise par la racine du carré moyen des gradients.\"\"\"\n    def __init__(self, params, lr=0.001, rho=0.9, eps=1e-8):\n        super().__init__(params, lr)\n        self.rho = rho\n        self.eps = eps\n        self.v = [np.zeros_like(p.data) for p in params]\n\n    def step(self):\n        for i, p in enumerate(self.params):\n            self.v[i] = self.rho * self.v[i] + (1 - self.rho) * p.grad**2\n            p.data -= self.lr / (np.sqrt(self.v[i]) + self.eps) * p.grad\n\n\nclass Adam(Optimizer):\n    \"\"\"Adam — Adaptive Moment Estimation (Kingma & Ba, 2014).\n    \n    Correction de biais intégrée pour les deux premiers moments.\n    Complexité: O(nb_params) par step.\n    \"\"\"\n    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0):\n        super().__init__(params, lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n        self.weight_decay = weight_decay\n        self.m = [np.zeros_like(p.data) for p in params]\n        self.v = [np.zeros_like(p.data) for p in params]\n        self.t = 0\n\n    def step(self):\n        self.t += 1\n        for i, p in enumerate(self.params):\n            g = p.grad + self.weight_decay * p.data\n            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * g**2\n            # Correction du biais\n            m_hat = self.m[i] / (1 - self.beta1**self.t)\n            v_hat = self.v[i] / (1 - self.beta2**self.t)\n            p.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n\n\nprint(\"✓ Optimiseurs implémentés: SGD, Momentum, RMSProp, Adam\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2b",
   "metadata": {},
   "outputs": [],
   "source": "# ─── BENCHMARK OPTIMISEURS : Rosenbrock function ─────────────────\n# f(x,y) = (1-x)^2 + 100*(y-x^2)^2\n# Minimum global en (1,1), très difficile à optimiser\n\ndef rosenbrock_loss(params):\n    x, y = params[0], params[1]\n    loss = (Tensor(1.0) - x)**2 + Tensor(100.0) * (y - x**2)**2\n    return loss\n\ndef run_optimizer_benchmark(OptimizerClass, kwargs, n_steps=500, name=\"\"):\n    x = Tensor(np.array(-1.5))\n    y = Tensor(np.array(0.5))\n    params = [x, y]\n    opt = OptimizerClass(params, **kwargs)\n    losses = []\n    for _ in range(n_steps):\n        opt.zero_grad()\n        loss = rosenbrock_loss(params)\n        loss.backward()\n        # reset grads sur copies\n        losses.append(float(loss.data))\n        opt.step()\n        # reset tensors for next iteration\n        x.grad = np.zeros_like(x.data)\n        y.grad = np.zeros_like(y.data)\n    return losses\n\nnp.random.seed(42)\nresults = {\n    'SGD (lr=1e-3)':       run_optimizer_benchmark(SGD,      {'lr': 1e-3}, name='SGD'),\n    'Momentum (lr=1e-3)':  run_optimizer_benchmark(Momentum, {'lr': 1e-3, 'beta': 0.9}),\n    'RMSProp (lr=1e-2)':   run_optimizer_benchmark(RMSProp,  {'lr': 1e-2}),\n    'Adam (lr=1e-2)':      run_optimizer_benchmark(Adam,     {'lr': 1e-2}),\n}\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n\nfor ax in axes:\n    for (name, losses), color in zip(results.items(), colors):\n        ax.plot(losses, label=name, color=color, lw=2)\n\naxes[0].set_title(\"Convergence — Rosenbrock (échelle linéaire)\")\naxes[0].set_xlabel(\"Itération\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].legend(fontsize=9)\naxes[0].set_ylim(0, 100)\n\naxes[1].set_yscale('log')\naxes[1].set_title(\"Convergence — Rosenbrock (échelle log)\")\naxes[1].set_xlabel(\"Itération\")\naxes[1].set_ylabel(\"Loss (log)\")\naxes[1].legend(fontsize=9)\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_optimizers.png', dpi=120, bbox_inches='tight')\nplt.show()\n\nfor name, losses in results.items():\n    print(f\"  {name:30s} : loss finale = {losses[-1]:.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-3a",
   "metadata": {},
   "source": "---\n## PARTIE 3 — Modèles ML From Scratch\n\n### Modèle 1 : Régression Logistique\n\n**Binary Cross-Entropy Loss** :  \nL = -1/n Σ [y log(σ(Wx+b)) + (1-y) log(1-σ(Wx+b))]\n\nFonction convexe → gradient descent converge vers le minimum global.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3a",
   "metadata": {},
   "outputs": [],
   "source": "class LogisticRegression:\n    \"\"\"\n    Régression logistique binaire.\n    - Loss: Binary Cross-Entropy (BCE)\n    - Gradient analytique via autodiff\n    - Convexe => convergence garantie vers minimum global\n    \"\"\"\n    def __init__(self, n_features, lr=0.1, weight_decay=0.0, optimizer='sgd'):\n        scale = np.sqrt(1.0 / n_features)\n        self.W = Tensor(np.random.randn(n_features, 1) * scale)\n        self.b = Tensor(np.zeros((1, 1)))\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.params = [self.W, self.b]\n\n        if optimizer == 'adam':\n            self.opt = Adam(self.params, lr=lr, weight_decay=weight_decay)\n        else:\n            self.opt = SGD(self.params, lr=lr, weight_decay=weight_decay)\n\n    def forward(self, X):\n        \"\"\"Sigmoid(X @ W + b)\"\"\"\n        logits = X @ self.W + self.b\n        prob = (logits * -1).exp() + Tensor(1.0)\n        return Tensor(1.0) / prob  # sigmoid\n\n    def bce_loss(self, probs, y):\n        \"\"\"Binary Cross-Entropy : -mean(y*log(p) + (1-y)*log(1-p))\"\"\"\n        eps = Tensor(1e-15)\n        pos = y * probs.log()\n        neg = (Tensor(1.0) - y) * (Tensor(1.0) - probs + eps).log()\n        return (pos + neg).mean() * Tensor(-1.0)\n\n    def train_step(self, X, y):\n        self.opt.zero_grad()\n        probs = self.forward(X)\n        loss = self.bce_loss(probs, y)\n        loss.backward()\n        self.opt.step()\n        return float(loss.data)\n\n    def predict(self, X):\n        probs = self.forward(X)\n        return (probs.data >= 0.5).astype(int)\n\n    def accuracy(self, X, y):\n        preds = self.predict(X)\n        return np.mean(preds == y.data.astype(int))\n\n\n# ─── Dataset synthétique non linéaire ─────────────────────\nnp.random.seed(42)\nn = 400\n# Deux gaussiennes séparables\nX0 = np.random.randn(n//2, 2) + np.array([-2, -1])\nX1 = np.random.randn(n//2, 2) + np.array([2, 1])\nX_data = np.vstack([X0, X1])\ny_data = np.array([0]*(n//2) + [1]*(n//2), dtype=float).reshape(-1, 1)\n\nidx = np.random.permutation(n)\nX_data, y_data = X_data[idx], y_data[idx]\nsplit = int(0.8 * n)\nX_tr, X_te = X_data[:split], X_data[split:]\ny_tr, y_te = y_data[:split], y_data[split:]\n\n# Normalisation\nmean_, std_ = X_tr.mean(0), X_tr.std(0)\nX_tr_n = (X_tr - mean_) / std_\nX_te_n = (X_te - mean_) / std_\n\nX_tr_t = Tensor(X_tr_n)\ny_tr_t = Tensor(y_tr)\n\n# Entraînement\nmodel_lr = LogisticRegression(n_features=2, lr=0.5, weight_decay=1e-4)\nlosses_lr, accs_lr = [], []\n\nfor epoch in range(200):\n    loss = model_lr.train_step(X_tr_t, y_tr_t)\n    acc = model_lr.accuracy(Tensor(X_te_n), Tensor(y_te))\n    losses_lr.append(loss)\n    accs_lr.append(acc)\n\nprint(f\"Régression Logistique — Loss finale: {losses_lr[-1]:.4f} | Acc test: {accs_lr[-1]:.4f}\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Courbe de loss\naxes[0].plot(losses_lr, color='steelblue', lw=2)\naxes[0].set_title(\"Convergence BCE Loss\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\n\n# Courbe d'accuracy\naxes[1].plot(accs_lr, color='green', lw=2)\naxes[1].set_title(\"Accuracy Test\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\naxes[1].set_ylim(0, 1.1)\n\n# Frontière de décision\nxx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\ngrid = np.c_[xx.ravel(), yy.ravel()]\nprobs_grid = model_lr.forward(Tensor(grid)).data.reshape(xx.shape)\naxes[2].contourf(xx, yy, probs_grid, levels=50, cmap='RdBu', alpha=0.8)\naxes[2].contour(xx, yy, probs_grid, levels=[0.5], colors='k', lw=2)\naxes[2].scatter(X_te_n[y_te.ravel()==0, 0], X_te_n[y_te.ravel()==0, 1], \n                c='blue', s=20, alpha=0.7, label='Classe 0')\naxes[2].scatter(X_te_n[y_te.ravel()==1, 0], X_te_n[y_te.ravel()==1, 1], \n                c='red', s=20, alpha=0.7, label='Classe 1')\naxes[2].set_title(\"Frontière de décision\")\naxes[2].legend(fontsize=8)\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_logistic.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-3b",
   "metadata": {},
   "source": "---\n### Modèle 2 : MLP Profond (Deep Neural Network)\n\nArchitecture complète :\n- **Dense layer** (Linear)\n- **ReLU / GELU**\n- **Softmax numérique stable**\n- **Cross-Entropy stable** (log-sum-exp trick)\n- **Xavier / He initialization**\n- **Batch training**\n\n**Complexité** :\n- Forward : O(n·d²) par couche\n- Backward : O(n·d²) par couche (même ordre)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3b",
   "metadata": {},
   "outputs": [],
   "source": "class Linear:\n    \"\"\"\n    Couche linéaire : y = x @ W + b\n    Initialisation He (pour ReLU) ou Xavier (pour Tanh/Sigmoid).\n    \"\"\"\n    def __init__(self, in_dim, out_dim, activation='relu', bias=True):\n        # Initialisation selon l'activation\n        if activation in ('relu', 'gelu'):\n            # Initialisation He\n            scale = np.sqrt(2.0 / in_dim)\n        else:\n            # Initialisation Xavier/Glorot\n            scale = np.sqrt(2.0 / (in_dim + out_dim))\n\n        self.W = Tensor(np.random.randn(in_dim, out_dim) * scale)\n        self.b = Tensor(np.zeros((1, out_dim))) if bias else None\n\n    def forward(self, x):\n        out = x @ self.W\n        if self.b is not None:\n            out = out + self.b\n        return out\n\n    @property\n    def params(self):\n        p = [self.W]\n        if self.b is not None:\n            p.append(self.b)\n        return p\n\n\nclass MLP:\n    \"\"\"\n    Multi-Layer Perceptron complet.\n    \n    Architecture: [input] -> [hidden layers] -> [output]\n    Activation: ReLU ou GELU\n    Loss: Cross-Entropy stable (log-sum-exp)\n    \"\"\"\n    def __init__(self, layer_dims, activation='relu', weight_decay=0.0,\n                 optimizer='adam', lr=1e-3):\n        assert activation in ('relu', 'gelu')\n        self.activation_name = activation\n        self.weight_decay = weight_decay\n        self.layers = []\n\n        for i in range(len(layer_dims) - 1):\n            act = activation if i < len(layer_dims) - 2 else 'none'\n            self.layers.append(Linear(layer_dims[i], layer_dims[i+1], activation=act))\n\n        all_params = []\n        for layer in self.layers:\n            all_params.extend(layer.params)\n\n        if optimizer == 'adam':\n            self.opt = Adam(all_params, lr=lr, weight_decay=weight_decay)\n        elif optimizer == 'sgd':\n            self.opt = SGD(all_params, lr=lr, weight_decay=weight_decay)\n        else:\n            self.opt = Momentum(all_params, lr=lr)\n\n    def _activate(self, x):\n        if self.activation_name == 'relu':\n            return x.relu()\n        elif self.activation_name == 'gelu':\n            return x.gelu()\n\n    def forward(self, x):\n        \"\"\"Passe forward complète.\"\"\"\n        h = x\n        for i, layer in enumerate(self.layers):\n            h = layer.forward(h)\n            if i < len(self.layers) - 1:\n                h = self._activate(h)\n        return h  # logits (avant softmax)\n\n    def stable_softmax_log(self, logits):\n        \"\"\"Log-softmax numérique stable (log-sum-exp trick).\"\"\"\n        # Soustrait le max pour la stabilité numérique\n        m = np.max(logits.data, axis=1, keepdims=True)\n        shifted = logits + Tensor(-m)\n        log_z = shifted.exp().sum(axis=1, keepdims=True).log()\n        return shifted - log_z\n\n    def cross_entropy_loss(self, logits, y_onehot):\n        \"\"\"Cross-Entropy stable : -mean(sum(y * log_softmax(logits)))\"\"\"\n        log_probs = self.stable_softmax_log(logits)\n        return (log_probs * y_onehot).sum(axis=1).mean() * Tensor(-1.0)\n\n    def train_step(self, X, y_onehot):\n        self.opt.zero_grad()\n        logits = self.forward(X)\n        loss = self.cross_entropy_loss(logits, y_onehot)\n        loss.backward()\n        self.opt.step()\n        return float(loss.data)\n\n    def predict(self, X):\n        logits = self.forward(Tensor(X))\n        return np.argmax(logits.data, axis=1)\n\n    def accuracy(self, X, y):\n        preds = self.predict(X)\n        return np.mean(preds == y)\n\n    def get_grad_norms(self):\n        \"\"\"Calcule la norme des gradients par couche.\"\"\"\n        norms = []\n        for layer in self.layers:\n            for p in layer.params:\n                norms.append(np.linalg.norm(p.grad))\n        return norms\n\n\ndef to_onehot(y, n_classes):\n    n = len(y)\n    oh = np.zeros((n, n_classes))\n    oh[np.arange(n), y.astype(int)] = 1\n    return oh\n\n\nprint(\"✓ MLP implémenté avec: Dense, ReLU/GELU, Softmax stable, Cross-Entropy stable\")\nprint(\"  Initialisation: He (ReLU) / Xavier (autre)\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3b2",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Dataset synthétique multiclasse (spirales) ──────────────────\nnp.random.seed(42)\nN = 100  # points par classe\nK = 3    # classes\nD = 2    # features\n\nX_spiral, y_spiral = [], []\nfor k in range(K):\n    ix = range(N * k, N * (k + 1))\n    r = np.linspace(0.0, 1, N)\n    t = np.linspace(k * 4, (k + 1) * 4, N) + np.random.randn(N) * 0.2\n    X_spiral.append(np.c_[r * np.sin(t), r * np.cos(t)])\n    y_spiral.append(np.full(N, k))\n\nX_spiral = np.vstack(X_spiral)\ny_spiral = np.hstack(y_spiral)\n\n# Shuffle et split\nidx = np.random.permutation(len(y_spiral))\nX_spiral, y_spiral = X_spiral[idx], y_spiral[idx]\nsplit = int(0.8 * len(y_spiral))\nXs_tr, Xs_te = X_spiral[:split], X_spiral[split:]\nys_tr, ys_te = y_spiral[:split], y_spiral[split:]\n\nXs_tr_oh = to_onehot(ys_tr, K)\n\n# ─── Entraînement MLP ────────────────────────────────────────────\nmlp = MLP([2, 64, 64, 3], activation='relu', lr=5e-3, weight_decay=1e-4)\n\nmlp_losses, mlp_accs, grad_norms_history = [], [], []\nn_epochs = 300\nbatch_size = 64\n\nfor epoch in range(n_epochs):\n    # Mini-batch SGD\n    perm = np.random.permutation(len(Xs_tr))\n    epoch_loss = 0\n    n_batches = 0\n    for start in range(0, len(Xs_tr), batch_size):\n        batch_idx = perm[start:start + batch_size]\n        Xb = Tensor(Xs_tr[batch_idx])\n        yb = Tensor(Xs_tr_oh[batch_idx])\n        loss = mlp.train_step(Xb, yb)\n        epoch_loss += loss\n        n_batches += 1\n\n    mlp_losses.append(epoch_loss / n_batches)\n    acc = mlp.accuracy(Xs_te, ys_te)\n    mlp_accs.append(acc)\n\n    # Stocker normes gradients\n    if epoch % 10 == 0:\n        norms = mlp.get_grad_norms()\n        grad_norms_history.append(norms)\n\nprint(f\"MLP [2->64->64->3] — Loss finale: {mlp_losses[-1]:.4f} | Acc test: {mlp_accs[-1]:.4f}\")\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(mlp_losses, color='darkorange', lw=2)\naxes[0].set_title(\"Loss Cross-Entropy MLP\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\n\naxes[1].plot(mlp_accs, color='green', lw=2)\naxes[1].set_title(\"Accuracy Test\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylim(0, 1.05)\naxes[1].axhline(y=1.0, color='gray', ls='--', alpha=0.5)\n\n# Frontière de décision\nres = 200\nxx, yy = np.meshgrid(np.linspace(-1.5, 1.5, res), np.linspace(-1.5, 1.5, res))\ngrid = np.c_[xx.ravel(), yy.ravel()]\npreds_grid = mlp.predict(grid).reshape(xx.shape)\naxes[2].contourf(xx, yy, preds_grid, alpha=0.4, cmap='Set2', levels=[-0.5, 0.5, 1.5, 2.5])\ncolors_cls = ['red', 'blue', 'green']\nfor k in range(K):\n    mask = ys_te == k\n    axes[2].scatter(Xs_te[mask, 0], Xs_te[mask, 1], c=colors_cls[k], s=25, \n                   label=f'Cls {k}', alpha=0.8)\naxes[2].set_title(\"Frontière de décision MLP\")\naxes[2].legend(fontsize=8)\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_mlp.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3b3",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Analyse gradient flow — Vanishing/Exploding ─────────────────\nprint(\"Analyse du flux de gradients...\")\n\ndef analyze_gradient_flow(depth, width, activation, n_steps=50):\n    \"\"\"\n    Analyse la norme du gradient par couche pour différentes profondeurs.\n    \"\"\"\n    dims = [2] + [width] * depth + [3]\n    model = MLP(dims, activation=activation, lr=1e-3)\n    \n    Xb = Tensor(np.random.randn(32, 2))\n    yb = Tensor(to_onehot(np.random.randint(0, 3, 32), 3))\n    \n    model.opt.zero_grad()\n    logits = model.forward(Xb)\n    loss = model.cross_entropy_loss(logits, yb)\n    loss.backward()\n    \n    norms = []\n    for layer in model.layers:\n        norms.append(np.linalg.norm(layer.W.grad))\n    return norms\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nconfigs = [\n    (5, 32, 'relu'),   (10, 32, 'relu'),\n    (5, 32, 'gelu'),   (10, 32, 'gelu'),\n]\ntitles = [\n    'ReLU, 5 couches',   'ReLU, 10 couches',\n    'GELU, 5 couches',   'GELU, 10 couches',\n]\n\nfor ax, (depth, width, act), title in zip(axes.ravel(), configs, titles):\n    norms = analyze_gradient_flow(depth, width, act)\n    bars = ax.bar(range(len(norms)), norms, color='steelblue', alpha=0.8)\n    ax.set_title(title)\n    ax.set_xlabel(\"Indice de couche (0=entrée → N=sortie)\")\n    ax.set_ylabel(\"||∇W||\")\n    ax.set_yscale('log')\n\nplt.suptitle(\"Flux de gradients par couche — Vanishing gradient analysis\", fontsize=13, y=1.01)\nplt.tight_layout()\nplt.savefig('/home/claude/fig_gradient_flow.png', dpi=120, bbox_inches='tight')\nplt.show()\n\nprint(\"Observation: ReLU aide à préserver le gradient vs réseaux très profonds\")\nprint(\"Risque de vanishing gradient sans normalisation pour profondeur > 10\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-3c",
   "metadata": {},
   "source": "---\n### Modèle 3 : Kernel Ridge Regression (Méthode à Noyau)\n\n**Formulation duale** :\n- α = (K + λI)⁻¹ y\n- Prédiction : f(x) = Σᵢ αᵢ k(xᵢ, x)\n\n**Noyau RBF** : k(x, x') = exp(-γ ||x - x'||²)\n\n**Complexité** :\n- Construction K : O(n²·d)\n- Inversion : O(n³) — facteur limitant !\n- Mémoire : O(n²) — stockage de la matrice de Gram\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3c",
   "metadata": {},
   "outputs": [],
   "source": "class KernelRidgeRegression:\n    \"\"\"\n    Kernel Ridge Regression with RBF kernel.\n    \n    Complexité:\n    - Fit: O(n^3) (inversion de la matrice de Gram)\n    - Predict: O(n_test * n_train)\n    - Mémoire: O(n^2)\n    \"\"\"\n    def __init__(self, gamma=1.0, lam=1.0):\n        self.gamma = gamma   # paramètre du noyau RBF\n        self.lam = lam       # régularisation L2\n        self.alpha = None\n        self.X_train = None\n\n    def rbf_kernel(self, X1, X2):\n        \"\"\"\n        Calcule la matrice de Gram K[i,j] = exp(-gamma * ||x_i - x_j||^2).\n        Complexité: O(n1 * n2 * d)\n        \"\"\"\n        # ||x_i - x_j||^2 = ||x_i||^2 + ||x_j||^2 - 2*x_i^T*x_j\n        sq1 = np.sum(X1**2, axis=1, keepdims=True)  # (n1, 1)\n        sq2 = np.sum(X2**2, axis=1, keepdims=True)  # (n2, 1)\n        cross = X1 @ X2.T                            # (n1, n2)\n        dist2 = sq1 + sq2.T - 2 * cross\n        return np.exp(-self.gamma * dist2)\n\n    def fit(self, X, y):\n        \"\"\"\n        Résout le système linéaire (K + λI)α = y.\n        Complexité: O(n^3) pour np.linalg.solve\n        \"\"\"\n        self.X_train = X.copy()\n        n = X.shape[0]\n        t0 = time.time()\n        K = self.rbf_kernel(X, X)                   # Matrice de Gram n×n\n        self.alpha = np.linalg.solve(               # O(n^3)\n            K + self.lam * np.eye(n), y)\n        self.fit_time = time.time() - t0\n\n    def predict(self, X_test):\n        \"\"\"Prédiction : f(x) = K_test @ alpha\"\"\"\n        K_test = self.rbf_kernel(X_test, self.X_train)  # (n_test, n_train)\n        return K_test @ self.alpha\n\n\n# ─── Régression sur données synthétiques nonlinéaires ────────────\nnp.random.seed(42)\nn_train = 200\n# Signal non linéaire : sin + bruit\nX_krr = np.linspace(-3, 3, n_train).reshape(-1, 1)\ny_krr = np.sin(X_krr.ravel()) + 0.2 * np.random.randn(n_train)\n\n# Split\nsplit_k = int(0.7 * n_train)\nXk_tr, Xk_te = X_krr[:split_k], X_krr[split_k:]\nyk_tr, yk_te = y_krr[:split_k], y_krr[split_k:]\n\n# Entraîner plusieurs modèles\nmodels_krr = {}\nfor gamma, lam in [(0.5, 0.01), (2.0, 0.01), (2.0, 1.0), (10.0, 0.001)]:\n    krr = KernelRidgeRegression(gamma=gamma, lam=lam)\n    krr.fit(Xk_tr, yk_tr)\n    ypred = krr.predict(Xk_te)\n    mse = np.mean((ypred - yk_te)**2)\n    models_krr[f'γ={gamma}, λ={lam}'] = (krr, mse)\n    print(f\"  KRR γ={gamma}, λ={lam}: MSE test = {mse:.4f}, fit_time = {krr.fit_time*1000:.2f}ms\")\n\nX_plot = np.linspace(-3, 3, 300).reshape(-1, 1)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfor ax, (label, (model, mse)) in zip(axes.ravel(), models_krr.items()):\n    y_plot = model.predict(X_plot)\n    ax.scatter(Xk_tr, yk_tr, s=15, alpha=0.5, c='steelblue', label='Train')\n    ax.scatter(Xk_te, yk_te, s=15, alpha=0.5, c='orange', label='Test')\n    ax.plot(X_plot, y_plot, 'r-', lw=2, label=f'KRR')\n    ax.plot(X_plot, np.sin(X_plot), 'g--', lw=1.5, alpha=0.7, label='Vrai signal')\n    ax.set_title(f'{label} | MSE={mse:.3f}')\n    ax.legend(fontsize=7)\n\nplt.suptitle(\"Kernel Ridge Regression — RBF Kernel\", fontsize=13)\nplt.tight_layout()\nplt.savefig('/home/claude/fig_krr.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-4",
   "metadata": {},
   "source": "---\n## PARTIE 4 — Dataset Réel : MNIST (chargement manuel)\n\nChargement des fichiers MNIST depuis leur format binaire IDX (sans sklearn).  \nFormat IDX : magic number + dimensions + données.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Génération d'un dataset MNIST-like (sans réseau) ────────────\n# Comme MNIST n'est pas disponible localement, on génère un dataset\n# synthétique de référence similaire à MNIST pour tester les modèles.\n\ndef load_mnist_or_synthetic(n_samples=2000, n_classes=10, n_features=784):\n    \"\"\"\n    Tente de charger MNIST depuis les fichiers IDX.\n    Si non disponible, génère un dataset synthétique de même dimension.\n    \"\"\"\n    def read_idx(filename):\n        with gzip.open(filename, 'rb') as f:\n            magic = struct.unpack('>I', f.read(4))[0]\n            n_dims = magic & 0xFF\n            dims = [struct.unpack('>I', f.read(4))[0] for _ in range(n_dims)]\n            data = np.frombuffer(f.read(), dtype=np.uint8)\n            return data.reshape(dims)\n\n    mnist_files = [\n        'train-images-idx3-ubyte.gz',\n        'train-labels-idx1-ubyte.gz',\n        't10k-images-idx3-ubyte.gz',\n        't10k-labels-idx1-ubyte.gz',\n    ]\n\n    # Vérifier si les fichiers MNIST existent\n    if all(os.path.exists(f) for f in mnist_files):\n        X_train = read_idx(mnist_files[0]).reshape(-1, 784) / 255.0\n        y_train = read_idx(mnist_files[1])\n        X_test = read_idx(mnist_files[2]).reshape(-1, 784) / 255.0\n        y_test = read_idx(mnist_files[3])\n        print(\"✓ MNIST chargé depuis fichiers IDX\")\n        return X_train, y_train, X_test, y_test\n    else:\n        print(\"! MNIST non disponible — génération d'un dataset synthétique de même dimension (784 features, 10 classes)\")\n        np.random.seed(42)\n        # Dataset synthétique inspiré de MNIST (PCA-like structure)\n        n_total = n_samples\n        X_synth = []\n        y_synth = []\n        for c in range(n_classes):\n            n_c = n_total // n_classes\n            # Chaque classe a un \"prototype\" dans l'espace 784d\n            proto = np.random.randn(n_features) * 0.3\n            proto[c * (n_features // n_classes): (c+1) * (n_features // n_classes)] += 2.0\n            Xc = proto + np.random.randn(n_c, n_features) * 0.5\n            Xc = np.clip(Xc, 0, 1)\n            X_synth.append(Xc)\n            y_synth.append(np.full(n_c, c))\n        X_synth = np.vstack(X_synth)\n        y_synth = np.hstack(y_synth)\n        perm = np.random.permutation(len(y_synth))\n        X_synth, y_synth = X_synth[perm], y_synth[perm]\n        split = int(0.8 * len(y_synth))\n        return X_synth[:split], y_synth[:split], X_synth[split:], y_synth[split:]\n\nX_train, y_train, X_test, y_test = load_mnist_or_synthetic(n_samples=2000)\nprint(f\"Dataset: X_train={X_train.shape}, y_train={y_train.shape}\")\nprint(f\"         X_test={X_test.shape}, y_test={y_test.shape}\")\nprint(f\"Classes: {np.unique(y_test)}\")\n\n# Visualiser quelques exemples\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.ravel()):\n    ax.imshow(X_train[i].reshape(int(X_train.shape[1]**0.5), -1), \n              cmap='gray', aspect='auto')\n    ax.set_title(f'Label: {y_train[i]}', fontsize=10)\n    ax.axis('off')\nplt.suptitle(\"Exemples du dataset (synthétique MNIST-like, 784 features)\", fontsize=12)\nplt.tight_layout()\nplt.savefig('/home/claude/fig_dataset.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-5",
   "metadata": {},
   "source": "---\n## PARTIE 5 — Expériences Sérieuses\n\n### 5.1 Comparaison Optimiseurs sur données réelles\n### 5.2 Impact du Learning Rate  \n### 5.3 Régularisation L2 — Overfitting\n### 5.4 Comparaison : Logistique vs MLP vs KRR\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5a",
   "metadata": {},
   "outputs": [],
   "source": "# ─── 5.1 : Comparaison optimiseurs sur données réelles ────────────\nprint(\"=\" * 60)\nprint(\"EXPÉRIENCE 5.1 : Comparaison optimiseurs\")\nprint(\"=\" * 60)\n\n# Réduire à 64 features pour accélérer (PCA manuelle approximative)\nnp.random.seed(42)\nn_components = 64  # pseudo-PCA\nU = np.random.randn(X_train.shape[1], n_components) / np.sqrt(n_components)\nXtr_red = X_train @ U\nXte_red = X_test @ U\n\n# Normalisation\nmu, sig = Xtr_red.mean(0), Xtr_red.std(0) + 1e-8\nXtr_red = (Xtr_red - mu) / sig\nXte_red = (Xte_red - mu) / sig\n\nn_classes_data = len(np.unique(y_train))\nYtr_oh = to_onehot(y_train, n_classes_data)\n\noptimizer_configs = [\n    ('SGD lr=0.1',    MLP, [64, 128, n_classes_data], {'optimizer': 'sgd', 'lr': 0.1}),\n    ('Momentum lr=0.05', MLP, [64, 128, n_classes_data], {'optimizer': 'momentum', 'lr': 0.05}),\n    ('Adam lr=1e-3',  MLP, [64, 128, n_classes_data], {'optimizer': 'adam', 'lr': 1e-3}),\n    ('Adam lr=1e-4',  MLP, [64, 128, n_classes_data], {'optimizer': 'adam', 'lr': 1e-4}),\n]\n\nexp_results = {}\nn_epochs_exp = 100\nbatch_sz = 128\n\nfor name, ModelClass, dims, kwargs in optimizer_configs:\n    np.random.seed(42)\n    model = ModelClass(dims, activation='relu', **kwargs)\n    losses_ep, accs_ep = [], []\n\n    for epoch in range(n_epochs_exp):\n        perm = np.random.permutation(len(Xtr_red))\n        ep_loss = 0\n        nb = 0\n        for start in range(0, len(Xtr_red), batch_sz):\n            idx_b = perm[start:start+batch_sz]\n            Xb = Tensor(Xtr_red[idx_b])\n            yb = Tensor(Ytr_oh[idx_b])\n            ep_loss += model.train_step(Xb, yb)\n            nb += 1\n        losses_ep.append(ep_loss / nb)\n        accs_ep.append(model.accuracy(Xte_red, y_test))\n\n    exp_results[name] = {'losses': losses_ep, 'accs': accs_ep}\n    print(f\"  {name:25s}: Acc finale = {accs_ep[-1]:.4f}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\ncolors_exp = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\nfor (name, data), c in zip(exp_results.items(), colors_exp):\n    ax1.plot(data['losses'], label=name, color=c, lw=2)\n    ax2.plot(data['accs'], label=name, color=c, lw=2)\n\nax1.set_title(\"Loss par epoch — différents optimiseurs\")\nax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Cross-Entropy Loss\")\nax1.legend(fontsize=9)\n\nax2.set_title(\"Accuracy Test — différents optimiseurs\")\nax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"Accuracy\")\nax2.legend(fontsize=9); ax2.set_ylim(0, 1.05)\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_exp_optimizers.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5b",
   "metadata": {},
   "outputs": [],
   "source": "# ─── 5.2 : Impact du Learning Rate ───────────────────────────────\nprint(\"=\" * 60)\nprint(\"EXPÉRIENCE 5.2 : Impact du Learning Rate\")\nprint(\"=\" * 60)\n\nlr_values = [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 0.5]\nlr_results = {}\n\nfor lr in lr_values:\n    np.random.seed(42)\n    model = MLP([64, 64, n_classes_data], activation='relu', lr=lr, optimizer='adam')\n    losses_lr_exp = []\n    for epoch in range(80):\n        perm = np.random.permutation(len(Xtr_red))\n        ep_loss = 0; nb = 0\n        for start in range(0, len(Xtr_red), batch_sz):\n            idx_b = perm[start:start+batch_sz]\n            Xb, yb = Tensor(Xtr_red[idx_b]), Tensor(Ytr_oh[idx_b])\n            ep_loss += model.train_step(Xb, yb); nb += 1\n        losses_lr_exp.append(ep_loss / nb)\n    lr_results[lr] = losses_lr_exp\n    print(f\"  lr={lr:.0e}: loss finale = {losses_lr_exp[-1]:.4f}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\ncmap_lr = plt.cm.plasma(np.linspace(0, 0.9, len(lr_values)))\n\nfor (lr, losses), c in zip(lr_results.items(), cmap_lr):\n    ax1.plot(losses, label=f'lr={lr:.0e}', color=c, lw=2)\n    ax2.plot(losses, label=f'lr={lr:.0e}', color=c, lw=2)\n\nax1.set_title(\"Convergence — différents learning rates\")\nax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\")\nax1.legend(fontsize=9)\nax1.set_ylim(0, 8)\n\nax2.set_yscale('log')\nax2.set_title(\"Convergence (échelle log)\")\nax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"Loss (log)\")\nax2.legend(fontsize=9)\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_lr_impact.png', dpi=120, bbox_inches='tight')\nplt.show()\n\nprint(\"Conclusion: lr=1e-3 à 1e-2 optimal pour Adam; lr trop grand => instabilité\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5c",
   "metadata": {},
   "outputs": [],
   "source": "# ─── 5.3 : Régularisation L2 et Overfitting ───────────────────────\nprint(\"=\" * 60)\nprint(\"EXPÉRIENCE 5.3 : Régularisation L2 — Overfitting\")\nprint(\"=\" * 60)\n\n# Petit dataset pour forcer l'overfitting\nn_small = 100\nXtr_small = Xtr_red[:n_small]\nYtr_small_oh = Ytr_oh[:n_small]\nytr_small = y_train[:n_small]\n\nlambda_values = [0.0, 1e-5, 1e-4, 1e-3, 1e-2, 0.1]\nreg_results = {}\n\nfor lam in lambda_values:\n    np.random.seed(42)\n    model = MLP([64, 128, 128, n_classes_data], activation='relu', \n                lr=1e-3, optimizer='adam', weight_decay=lam)\n    tr_accs, te_accs = [], []\n\n    for epoch in range(150):\n        perm = np.random.permutation(n_small)\n        for start in range(0, n_small, 32):\n            idx_b = perm[start:start+32]\n            Xb, yb = Tensor(Xtr_small[idx_b]), Tensor(Ytr_small_oh[idx_b])\n            model.train_step(Xb, yb)\n\n        tr_accs.append(model.accuracy(Xtr_small, ytr_small))\n        te_accs.append(model.accuracy(Xte_red, y_test))\n\n    reg_results[lam] = (tr_accs, te_accs)\n    print(f\"  λ={lam:.0e}: train={tr_accs[-1]:.3f}, test={te_accs[-1]:.3f}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\ncmap_reg = plt.cm.viridis(np.linspace(0, 0.9, len(lambda_values)))\n\nfor (lam, (tr, te)), c in zip(reg_results.items(), cmap_reg):\n    axes[0].plot(tr, color=c, lw=2, label=f'λ={lam:.0e}')\n    axes[1].plot(te, color=c, lw=2, label=f'λ={lam:.0e}')\n    axes[2].plot([t - e for t, e in zip(tr, te)], color=c, lw=2, label=f'λ={lam:.0e}')\n\naxes[0].set_title(\"Accuracy Train\"); axes[0].legend(fontsize=8)\naxes[1].set_title(\"Accuracy Test\"); axes[1].legend(fontsize=8)\naxes[2].set_title(\"Gap Overfitting (Train-Test)\")\naxes[2].axhline(y=0, color='k', ls='--', alpha=0.5)\naxes[2].legend(fontsize=8)\n\nfor ax in axes:\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"Accuracy\")\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_regularization.png', dpi=120, bbox_inches='tight')\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5d",
   "metadata": {},
   "outputs": [],
   "source": "# ─── 5.4 : Comparaison Modèles — Logistic vs MLP vs KRR ──────────\nprint(\"=\" * 60)\nprint(\"EXPÉRIENCE 5.4 : Comparaison Logistic vs MLP vs KRR\")\nprint(\"=\" * 60)\n\n# Utiliser dataset spirale (multiclasse, 2D) pour visualisation\nfrom time import time\n\nresults_compare = {}\n\n# ── Logistic (multiclasse via OvR) ──\nclass MulticlassLogistic:\n    def __init__(self, n_features, n_classes, lr=0.1):\n        self.classifiers = [\n            LogisticRegression(n_features, lr=lr)\n            for _ in range(n_classes)\n        ]\n        self.n_classes = n_classes\n\n    def train(self, X, y, n_epochs=200):\n        y_arr = y.astype(int)\n        Xt = Tensor(X)\n        for k, clf in enumerate(self.classifiers):\n            yk = Tensor((y_arr == k).astype(float).reshape(-1, 1))\n            for _ in range(n_epochs):\n                clf.train_step(Xt, yk)\n\n    def predict(self, X):\n        Xt = Tensor(X)\n        probs = np.column_stack([clf.forward(Xt).data.ravel() \n                                  for clf in self.classifiers])\n        return np.argmax(probs, axis=1)\n\n    def accuracy(self, X, y):\n        return np.mean(self.predict(X) == y.astype(int))\n\nnp.random.seed(42)\n\n# Données spirale\nt0 = time()\nmlr = MulticlassLogistic(2, K, lr=0.3)\nmlr.train(Xs_tr, ys_tr, n_epochs=150)\nt_mlr = time() - t0\nacc_mlr = mlr.accuracy(Xs_te, ys_te)\nresults_compare['Logistic (OvR)'] = {'acc': acc_mlr, 'time': t_mlr}\nprint(f\"  Logistic OvR    : Acc={acc_mlr:.4f}, Time={t_mlr:.3f}s\")\n\n# MLP (déjà entraîné)\nt0 = time()\nmlp2 = MLP([2, 64, 64, K], activation='relu', lr=5e-3, optimizer='adam')\nfor epoch in range(300):\n    perm = np.random.permutation(len(Xs_tr))\n    for start in range(0, len(Xs_tr), 32):\n        ib = perm[start:start+32]\n        mlp2.train_step(Tensor(Xs_tr[ib]), Tensor(to_onehot(ys_tr[ib], K)))\nt_mlp = time() - t0\nacc_mlp = mlp2.accuracy(Xs_te, ys_te)\nresults_compare['MLP [2→64→64→3]'] = {'acc': acc_mlp, 'time': t_mlp}\nprint(f\"  MLP [2→64→64→3] : Acc={acc_mlp:.4f}, Time={t_mlp:.3f}s\")\n\n# KRR (classification — utiliser K noyaux OvR en régression)\nclass KernelClassifier:\n    def __init__(self, gamma=1.0, lam=0.1):\n        self.models = None\n        self.gamma = gamma\n        self.lam = lam\n\n    def fit(self, X, y):\n        n_cls = len(np.unique(y))\n        self.models = []\n        for k in range(n_cls):\n            yk = (y == k).astype(float)\n            m = KernelRidgeRegression(gamma=self.gamma, lam=self.lam)\n            m.fit(X, yk)\n            self.models.append(m)\n\n    def predict(self, X):\n        scores = np.column_stack([m.predict(X) for m in self.models])\n        return np.argmax(scores, axis=1)\n\n    def accuracy(self, X, y):\n        return np.mean(self.predict(X) == y.astype(int))\n\nt0 = time()\nkc = KernelClassifier(gamma=2.0, lam=0.05)\nkc.fit(Xs_tr, ys_tr)\nt_krr = time() - t0\nacc_krr = kc.accuracy(Xs_te, ys_te)\nresults_compare['KRR (OvR)'] = {'acc': acc_krr, 'time': t_krr}\nprint(f\"  KRR (OvR)       : Acc={acc_krr:.4f}, Time={t_krr:.3f}s\")\n\n# Visualisation comparée\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nmodel_list = [('Logistic OvR', mlr, lambda X: mlr.predict(X)),\n              ('MLP', mlp2, lambda X: mlp2.predict(X)),\n              ('KRR', kc, lambda X: kc.predict(X))]\n\nfor ax, (name, model, pred_fn) in zip(axes, model_list):\n    res2 = 150\n    xx2, yy2 = np.meshgrid(np.linspace(-1.5, 1.5, res2), np.linspace(-1.5, 1.5, res2))\n    grid2 = np.c_[xx2.ravel(), yy2.ravel()]\n    preds2 = pred_fn(grid2).reshape(xx2.shape)\n    acc_model = results_compare.get(name, results_compare.get('MLP [2→64→64→3]', {'acc': 0}))['acc']\n    ax.contourf(xx2, yy2, preds2, alpha=0.35, cmap='Set2', levels=[-0.5, 0.5, 1.5, 2.5])\n    for k in range(K):\n        mask = ys_te == k\n        ax.scatter(Xs_te[mask, 0], Xs_te[mask, 1], c=colors_cls[k], s=25, alpha=0.8)\n    ax.set_title(f'{name}\\nAcc={results_compare.get(name, results_compare.get(\"KRR (OvR)\", {\"acc\":acc_krr}))[\"acc\"]:.3f}')\n\nplt.suptitle(\"Comparaison des modèles — Dataset Spirale\", fontsize=13)\nplt.tight_layout()\nplt.savefig('/home/claude/fig_model_comparison.png', dpi=120, bbox_inches='tight')\nplt.show()\n\n# Tableau récapitulatif\nprint(\"\\n── Tableau comparatif ──────────────────────────────────\")\nprint(f\"{'Modèle':<22} {'Accuracy':>10} {'Temps (s)':>12} {'Complexité fit':<20}\")\nprint(\"-\" * 68)\ncomplexities = ['O(n·d) epochs', 'O(n·d²) epochs', 'O(n³)']\nfor (name, data), comp in zip(results_compare.items(), complexities):\n    print(f\"{name:<22} {data['acc']:>10.4f} {data['time']:>12.4f} {comp:<20}\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-6",
   "metadata": {},
   "source": "---\n## PARTIE 6 — Analyse de Complexité Empirique\n\nOn mesure les temps d'exécution réels en fonction de n et d pour valider les complexités théoriques.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# ─── Complexité empirique : Autodiff ─────────────────────────────\nprint(\"=\" * 60)\nprint(\"ANALYSE DE COMPLEXITÉ EMPIRIQUE\")\nprint(\"=\" * 60)\n\n# 1. Autodiff — O(|graph|)\ndepths_test = [2, 4, 6, 8, 10, 12, 15]\nautodiff_times = []\nfor d in depths_test:\n    times = []\n    for _ in range(5):\n        x_ = Tensor(np.random.randn(32, 16))\n        layer_ = x_\n        Ws = [Tensor(np.random.randn(16, 16)) for _ in range(d)]\n        t0 = time()\n        for W_ in Ws:\n            layer_ = (layer_ @ W_).relu()\n        loss_ = layer_.mean()\n        loss_.backward()\n        times.append(time() - t0)\n    autodiff_times.append(np.mean(times) * 1000)\n\n# 2. MLP forward — O(n * d^2)\nn_samples_test = [50, 100, 200, 500, 1000]\nmlp_fwd_times = []\nfor n in n_samples_test:\n    model_test = MLP([32, 64, 64, 10], activation='relu', lr=1e-3)\n    X_test_cplx = np.random.randn(n, 32)\n    times = []\n    for _ in range(5):\n        t0 = time()\n        _ = model_test.forward(Tensor(X_test_cplx))\n        times.append(time() - t0)\n    mlp_fwd_times.append(np.mean(times) * 1000)\n\n# 3. KRR — O(n^3)\nns_krr = [50, 100, 200, 300, 500]\nkrr_times = []\nfor n in ns_krr:\n    Xk = np.random.randn(n, 10)\n    yk = np.random.randn(n)\n    times = []\n    for _ in range(3):\n        krr_test = KernelRidgeRegression(gamma=1.0, lam=0.1)\n        t0 = time()\n        krr_test.fit(Xk, yk)\n        times.append(time() - t0)\n    krr_times.append(np.mean(times) * 1000)\n\n# Visualisation\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Autodiff\naxes[0].plot(depths_test, autodiff_times, 'o-', color='steelblue', lw=2, ms=8)\ncoeffs = np.polyfit(depths_test, autodiff_times, 1)\nfit_line = np.poly1d(coeffs)(depths_test)\naxes[0].plot(depths_test, fit_line, '--', color='red', alpha=0.7, label=f'Fit linéaire')\naxes[0].set_title(\"Autodiff — Temps vs Profondeur\")\naxes[0].set_xlabel(\"Profondeur du réseau\"); axes[0].set_ylabel(\"Temps (ms)\")\naxes[0].legend()\naxes[0].text(0.05, 0.95, \"O(depth) ✓\", transform=axes[0].transAxes, \n             color='green', fontsize=11, fontweight='bold', va='top')\n\n# MLP forward\naxes[1].plot(n_samples_test, mlp_fwd_times, 'o-', color='darkorange', lw=2, ms=8)\ncoeffs2 = np.polyfit(n_samples_test, mlp_fwd_times, 1)\naxes[1].plot(n_samples_test, np.poly1d(coeffs2)(n_samples_test), '--r', alpha=0.7, label='Fit O(n)')\naxes[1].set_title(\"MLP Forward — Temps vs n\")\naxes[1].set_xlabel(\"n samples\"); axes[1].set_ylabel(\"Temps (ms)\")\naxes[1].legend()\naxes[1].text(0.05, 0.95, \"O(n·d²) ✓\", transform=axes[1].transAxes,\n             color='green', fontsize=11, fontweight='bold', va='top')\n\n# KRR\naxes[2].plot(ns_krr, krr_times, 'o-', color='purple', lw=2, ms=8)\nn_arr = np.array(ns_krr)\n# Fit cubique\ncoeffs3 = np.polyfit(np.log(n_arr), np.log(np.array(krr_times)), 1)\naxes[2].plot(n_arr, krr_times, 'o-', color='purple', lw=2)\naxes[2].set_yscale('log'); axes[2].set_xscale('log')\naxes[2].set_title(f\"KRR Fit — Temps vs n (log-log)\")\naxes[2].set_xlabel(\"n samples\"); axes[2].set_ylabel(\"Temps (ms)\")\naxes[2].text(0.05, 0.95, f\"Pente log-log ≈ {coeffs3[0]:.2f} (théo: 3.0)\",\n             transform=axes[2].transAxes, color='green', fontsize=9, fontweight='bold', va='top')\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_complexity.png', dpi=120, bbox_inches='tight')\nplt.show()\n\nprint(f\"Pente empirique KRR (log-log): {coeffs3[0]:.2f} (attendu: ~3.0 = O(n^3))\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-bonus",
   "metadata": {},
   "source": "---\n## BONUS — Fonctionnalités Avancées (niveau M2+)\n\n### B1 : Gradient Clipping\n### B2 : Learning Rate Scheduler  \n### B3 : Early Stopping  \n### B4 : Newton Approximé (Diagonal Hessian)\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-bonus",
   "metadata": {},
   "outputs": [],
   "source": "# ─── BONUS B1 : Gradient Clipping ───────────────────────────────\ndef gradient_clipping(params, max_norm=1.0):\n    \"\"\"\n    Clip les gradients pour éviter l'explosion.\n    Si ||g|| > max_norm, scale g pour que ||g|| = max_norm.\n    \"\"\"\n    total_norm = 0.0\n    for p in params:\n        total_norm += np.sum(p.grad**2)\n    total_norm = np.sqrt(total_norm)\n    clip_coef = max_norm / max(total_norm, max_norm)\n    for p in params:\n        p.grad *= clip_coef\n    return total_norm\n\n\n# ─── BONUS B2 : Learning Rate Scheduler ─────────────────────────\nclass CosineAnnealingScheduler:\n    \"\"\"\n    Cosine Annealing LR Scheduler (Loshchilov & Hutter, 2016).\n    lr(t) = lr_min + 0.5*(lr_max - lr_min)*(1 + cos(pi*t/T))\n    \"\"\"\n    def __init__(self, optimizer, T_max, lr_min=1e-6):\n        self.optimizer = optimizer\n        self.T_max = T_max\n        self.lr_min = lr_min\n        self.lr_max = optimizer.lr\n        self.t = 0\n\n    def step(self):\n        self.t += 1\n        lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (\n            1 + np.cos(np.pi * self.t / self.T_max))\n        self.optimizer.lr = lr\n        return lr\n\n\n# ─── BONUS B3 : Early Stopping ───────────────────────────────────\nclass EarlyStopping:\n    \"\"\"\n    Arrêt automatique quand la loss de validation ne s'améliore plus.\n    Critère théorique : generalization loss > patience * seuil.\n    \"\"\"\n    def __init__(self, patience=10, delta=1e-4):\n        self.patience = patience\n        self.delta = delta\n        self.best_loss = np.inf\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n        return self.stop\n\n\n# ─── Demo : entraînement avec scheduler + early stopping ─────────\nnp.random.seed(42)\nmodel_bonus = MLP([64, 128, 64, n_classes_data], activation='relu', lr=5e-3, optimizer='adam')\nscheduler = CosineAnnealingScheduler(model_bonus.opt, T_max=100, lr_min=1e-5)\nearly_stop = EarlyStopping(patience=15)\n\nlosses_bonus, lrs_bonus = [], []\n\nfor epoch in range(200):\n    perm = np.random.permutation(len(Xtr_red))\n    ep_loss = 0; nb = 0\n    for start in range(0, len(Xtr_red), 64):\n        idx_b = perm[start:start+64]\n        Xb, yb = Tensor(Xtr_red[idx_b]), Tensor(Ytr_oh[idx_b])\n        ep_loss += model_bonus.train_step(Xb, yb); nb += 1\n    ep_loss /= nb\n\n    losses_bonus.append(ep_loss)\n    lr_now = scheduler.step()\n    lrs_bonus.append(lr_now)\n\n    if early_stop(ep_loss):\n        print(f\"Early stopping déclenché à l'epoch {epoch}\")\n        break\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nax1.plot(losses_bonus, 'steelblue', lw=2)\nax1.set_title(\"Loss avec Cosine Annealing + Early Stopping\")\nax1.set_xlabel(\"Epoch\"); ax1.set_ylabel(\"Loss\")\n\nax2.plot(lrs_bonus, 'darkorange', lw=2)\nax2.set_title(\"Learning Rate — Cosine Annealing\")\nax2.set_xlabel(\"Epoch\"); ax2.set_ylabel(\"LR\")\n\nplt.tight_layout()\nplt.savefig('/home/claude/fig_bonus_scheduler.png', dpi=120, bbox_inches='tight')\nplt.show()\n\nprint(f\"Acc finale (avec bonus): {model_bonus.accuracy(Xte_red, y_test):.4f}\")\nprint(\"✓ Gradient Clipping, Cosine LR Scheduler, Early Stopping implémentés\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "md-summary",
   "metadata": {},
   "source": "---\n## Résumé Final\n\n| Composant | Implémenté | Complexité |\n|-----------|-----------|-----------|\n| Autodiff Engine | ✅ Tensor + graphe dynamique | O(\\|graph\\|) |\n| SGD / Momentum | ✅ | O(p) par step |\n| RMSProp / Adam | ✅ biais corrigé | O(p) par step |\n| Régression Logistique | ✅ BCE + convexité | O(n·d) par epoch |\n| MLP Profond | ✅ ReLU/GELU, He/Xavier | O(n·d²) par epoch |\n| Kernel Ridge Regression | ✅ RBF, Gram matrix | O(n³) fit |\n| Dataset (MNIST-like) | ✅ chargement IDX | — |\n| Expériences | ✅ optimiseurs, LR, reg, comparaison | — |\n| Analyse complexité | ✅ empirique + théorique | — |\n| BONUS: Scheduler, ES, Clipping | ✅ | — |\n\n**Auteur** : AHNANI Ali — Projet M2 Machine Learning\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"RÉSUMÉ DU PROJET\")\nprint(\"=\" * 60)\nprint()\nprint(\"✓ PARTIE 1 : Autodiff Engine\")\nprint(\"  - Classe Tensor avec graphe dynamique\")\nprint(\"  - Backward automatique (tri topologique)\")\nprint(\"  - add, mul, matmul, exp, log, relu, gelu, sum, mean, reshape, transpose\")\nprint()\nprint(\"✓ PARTIE 2 : Optimiseurs\")\nprint(\"  - SGD, Momentum, RMSProp, Adam\")\nprint(\"  - Benchmark Rosenbrock\")\nprint()\nprint(\"✓ PARTIE 3 : Modèles ML\")\nprint(\"  - Régression Logistique (BCE, convexe)\")\nprint(\"  - MLP Profond (ReLU/GELU, He/Xavier, batch training)\")\nprint(\"  - Kernel Ridge Regression (RBF, Gram, O(n^3))\")\nprint()\nprint(\"✓ PARTIE 4 : Dataset\")\nprint(\"  - Chargement IDX MNIST (ou synthétique 784d)\")\nprint(\"  - Dataset spirale multiclasse\")\nprint()\nprint(\"✓ PARTIE 5 : Expériences\")\nprint(\"  - Comparaison optimiseurs\")\nprint(\"  - Impact learning rate\")\nprint(\"  - Régularisation L2 / overfitting\")\nprint(\"  - Logistic vs MLP vs KRR\")\nprint()\nprint(\"✓ PARTIE 6 : Complexité empirique\")\nprint(\"  - Autodiff O(depth)\")\nprint(\"  - MLP O(n)\")\nprint(\"  - KRR O(n^3) confirmé\")\nprint()\nprint(\"✓ BONUS : Gradient Clipping, Cosine Scheduler, Early Stopping\")\nprint()\nprint(\"Projet M2 — AHNANI Ali\")\n"
  }
 ]
}