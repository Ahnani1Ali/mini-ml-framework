# üß† Mini ML Framework ‚Äî From Scratch

> **Autodiff ¬∑ Optimiseurs ¬∑ MLP ¬∑ Kernel Ridge ¬∑ MNIST**  
> Impl√©mentation compl√®te d'un framework de Machine Learning en **Python + NumPy uniquement** ‚Äî sans PyTorch, TensorFlow ni scikit-learn.

**Auteur :** AHNANI Ali

---

## Vue d'ensemble

Ce projet construit un pipeline ML moderne enti√®rement from scratch, de la diff√©rentiation automatique jusqu'√† l'entra√Ænement sur donn√©es r√©elles. Chaque brique algorithmique est impl√©ment√©e √† partir des math√©matiques, avec une analyse de complexit√© empiriquement valid√©e.

```
Tensor (autodiff) ‚Üí Optimiseur ‚Üí Mod√®le ‚Üí Donn√©es ‚Üí Exp√©riences ‚Üí Rapport
```

---

## Architecture

```
mini-ml-framework/
‚îÇ
‚îú‚îÄ‚îÄ autograd/
‚îÇ   ‚îî‚îÄ‚îÄ tensor.py            # Classe Tensor + graphe computationnel + backward
‚îÇ
‚îú‚îÄ‚îÄ optim/
‚îÇ   ‚îú‚îÄ‚îÄ sgd.py               # SGD + Momentum
‚îÇ   ‚îú‚îÄ‚îÄ rmsprop.py           # RMSProp
‚îÇ   ‚îî‚îÄ‚îÄ adam.py              # Adam (correction biais)
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ linear.py            # Couche Dense
‚îÇ   ‚îú‚îÄ‚îÄ activations.py       # ReLU, GELU, Softmax stable
‚îÇ   ‚îú‚îÄ‚îÄ losses.py            # BCE, CrossEntropy (log-sum-exp)
‚îÇ   ‚îú‚îÄ‚îÄ logistic.py          # R√©gression Logistique
‚îÇ   ‚îú‚îÄ‚îÄ mlp.py               # MLP Profond (Xavier/He init)
‚îÇ   ‚îî‚îÄ‚îÄ kernel.py            # Kernel Ridge Regression (RBF, Gram O(n¬≥))
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ mnist_loader.py      # Chargement MNIST format IDX binaire
‚îÇ   ‚îî‚îÄ‚îÄ synthetic.py         # Datasets synth√©tiques (spirale, gaussiennes)
‚îÇ
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py           # Boucle d'entra√Ænement mini-batch
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py         # Cosine Annealing LR
‚îÇ   ‚îî‚îÄ‚îÄ early_stopping.py    # Early Stopping (crit√®re Prechelt)
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ init.py              # Xavier / He initialization
‚îÇ   ‚îú‚îÄ‚îÄ grad_clip.py         # Gradient Clipping
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py           # Accuracy, MSE
‚îÇ
‚îú‚îÄ‚îÄ experiments/             # Scripts de reproduction des exp√©riences
‚îú‚îÄ‚îÄ notebooks/               # Notebook interactif complet
‚îú‚îÄ‚îÄ figures/                 # Figures g√©n√©r√©es
‚îî‚îÄ‚îÄ report/                  # Rapport LaTeX + PDF
```

---

## Contenu d√©taill√©

### Partie 1 ‚Äî Moteur Autodiff

Moteur de diff√©rentiation automatique en **mode reverse** (backpropagation g√©n√©ralis√©e).

- Classe `Tensor` avec graphe computationnel **dynamique** (define-by-run)
- **Tri topologique** + propagation backward automatique
- Accumulation de gradients (gestion des n≈ìuds r√©utilis√©s)
- Gestion du **broadcasting NumPy** dans les gradients

**Op√©rations support√©es :**

| Op√©ration | Forward | Backward |
|-----------|---------|----------|
| `add`, `mul` | `a+b`, `a*b` | r√®gle produit |
| `matmul` | `A @ B` | `G @ B·µÄ`, `A·µÄ @ G` |
| `exp`, `log` | `eÀ£`, `log x` | `eÀ£`, `1/x` |
| `relu`, `gelu` | `max(0,x)`, `x¬∑œÉ(1.702x)` | d√©riv√©es analytiques |
| `sum`, `mean` | r√©ductions | broadcast inverse |
| `pow`, `reshape`, `transpose` | ‚Äî | r√®gle puissance, reshape inverse |

**Complexit√© :** Temps `O(|graph|)` ¬∑ M√©moire `O(|graph|)`

```python
from autograd.tensor import Tensor

x = Tensor([[1.0, 2.0], [3.0, 4.0]])
W = Tensor([[0.5, -1.0], [1.0, 0.5]])
b = Tensor([[0.1, 0.1]])

out = (x @ W + b).relu().mean()
out.backward()

print(W.grad)  # ‚àÇloss/‚àÇW calcul√© automatiquement
```

---

### Partie 2 ‚Äî Optimiseurs

| Optimiseur | Mise √† jour | M√©moire | Adaptatif |
|-----------|------------|---------|-----------|
| SGD | `Œ∏ ‚Üê Œ∏ ‚àí Œ∑‚àáL` | O(p) | Non |
| Momentum | `v ‚Üê Œ≤v + ‚àáL`, `Œ∏ ‚Üê Œ∏ ‚àí Œ∑v` | O(2p) | Non |
| RMSProp | normalisation par `‚àöv` | O(2p) | Oui |
| Adam | moments 1 et 2 + correction biais | O(3p) | Oui |

```python
from optim.adam import Adam

optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)
optimizer.zero_grad()
loss.backward()
optimizer.step()
```

---

### Partie 3 ‚Äî Mod√®les

#### R√©gression Logistique
- Loss **Binary Cross-Entropy** (d√©riv√©e du maximum de vraisemblance)
- Fonction **convexe** ‚Üí convergence vers le minimum global garantie
- Gradient via autodiff : `‚àÇL/‚àÇw = (1/n) X·µÄ(œÉ(Xw) ‚àí y)`

#### MLP Profond
- Couches **Dense** encha√Æn√©es avec activations non-lin√©aires
- Activations : **ReLU** et **GELU** (approximation sigmo√Øde)
- Initialisation **He** (ReLU) et **Xavier/Glorot** (autre)
- **Softmax num√©rique stable** via log-sum-exp trick
- Analyse du **vanishing/exploding gradient** par couche

```python
from models.mlp import MLP

model = MLP(
    layer_dims=[784, 256, 128, 10],
    activation='relu',
    weight_decay=1e-4
)
```

#### Kernel Ridge Regression
- Noyau **RBF** : `k(x, x') = exp(‚àíŒ≥‚Äñx ‚àí x'‚Äñ¬≤)`
- **Th√©or√®me du repr√©sentant** ‚Üí solution `Œ± = (K + ŒªI)‚Åª¬πy`
- Construction efficace de la **matrice de Gram** via identit√© `‚Äñx‚àíx'‚Äñ¬≤ = ‚Äñx‚Äñ¬≤ + ‚Äñx'‚Äñ¬≤ ‚àí 2x·µÄx'`
- Complexit√© : **O(n¬≥)** fit ¬∑ **O(n¬≤)** m√©moire

```python
from models.kernel import KernelRidgeRegression

krr = KernelRidgeRegression(gamma=2.0, lam=0.1)
krr.fit(X_train, y_train)
y_pred = krr.predict(X_test)
```

---

### Partie 4 ‚Äî Dataset MNIST

Chargement des fichiers MNIST au **format binaire IDX** sans aucune d√©pendance externe :

```python
from data.mnist_loader import load_mnist

X_train, y_train, X_test, y_test = load_mnist('path/to/mnist/')
# X_train : (60000, 784), valeurs normalis√©es dans [0, 1]
```

Format IDX : `[magic 4B] [dims 4B√ón] [donn√©es uint8 big-endian]`

---

### Partie 5 ‚Äî Exp√©riences

Toutes les exp√©riences sont reproductibles avec un seed fix√© (`np.random.seed(42)`).

| Exp√©rience | Script | R√©sultat |
|-----------|--------|---------|
| Comparaison optimiseurs | `experiments/exp_optimizers.py` | Adam converge ~3√ó plus vite que SGD |
| Impact learning rate | `experiments/exp_lr.py` | Fen√™tre optimale `Œ∑ ‚àà [1e-3, 1e-2]` pour Adam |
| R√©gularisation L2 | `experiments/exp_regularization.py` | `Œª ‚âà 1e-4` r√©duit le gap train-test |
| Logistic vs MLP vs KRR | `experiments/exp_models.py` | MLP et KRR dominent sur non-lin√©aire |
| Benchmark complexit√© | `experiments/exp_complexity.py` | Pente log-log KRR ‚âà 2.9 ‚âà O(n¬≥) |

---

### Partie 6 ‚Äî Analyse de Complexit√©

| Composant | Temps | M√©moire |
|-----------|-------|---------|
| Autodiff (forward + backward) | O(\|graph\|) | O(\|graph\|) |
| MLP ‚Äî L couches, largeur d, batch n | O(L¬∑n¬∑d¬≤) | O(L¬∑n¬∑d) |
| Adam ‚Äî p param√®tres | O(p) / step | O(3p) |
| KRR ‚Äî fit | **O(n¬≥)** | O(n¬≤) |
| KRR ‚Äî predict | O(n\_test ¬∑ n) | O(n\_test ¬∑ n) |

---

### Fonctionnalit√©s Avanc√©es

#### Gradient Clipping
```python
from utils.grad_clip import gradient_clipping

total_norm = gradient_clipping(model.parameters(), max_norm=1.0)
```

#### Cosine Annealing LR Scheduler
```python
from training.scheduler import CosineAnnealingScheduler

scheduler = CosineAnnealingScheduler(optimizer, T_max=100, lr_min=1e-6)
# lr(t) = lr_min + 0.5*(lr_max - lr_min)*(1 + cos(œÄ¬∑t/T))
scheduler.step()
```

#### Early Stopping
```python
from training.early_stopping import EarlyStopping

es = EarlyStopping(patience=15, delta=1e-4)
if es(val_loss):
    print("Arr√™t anticip√©")
    break
```

---

## D√©marrage rapide

### Pr√©requis

```
Python >= 3.8
numpy >= 1.21
matplotlib >= 3.4
```

### Installation

```bash
git clone https://github.com/ton-username/mini-ml-framework.git
cd mini-ml-framework
pip install -r requirements.txt
```

### Lancer le notebook

```bash
jupyter notebook notebooks/mini_framework_ml_AHNANI_Ali.ipynb
```

### Reproduire une exp√©rience

```bash
python experiments/exp_optimizers.py
python experiments/exp_models.py
python experiments/exp_complexity.py
```

### Exemple minimal end-to-end

```python
import numpy as np
from autograd.tensor import Tensor
from models.mlp import MLP
from optim.adam import Adam
from utils.metrics import accuracy

# Donn√©es
X = np.random.randn(200, 4)
y = (X[:, 0] + X[:, 1] > 0).astype(int)

# Mod√®le
model = MLP([4, 32, 2], activation='relu')
optimizer = Adam(model.parameters(), lr=1e-3)

# Entra√Ænement
for epoch in range(100):
    optimizer.zero_grad()
    logits = model.forward(Tensor(X))
    loss = model.ce_loss(logits, Tensor(to_onehot(y, 2)))
    loss.backward()
    optimizer.step()

print(f"Accuracy : {accuracy(model.predict(X), y):.4f}")
```

---

## R√©sultats

| Mod√®le | Dataset Spirale (3 cls) | Temps fit |
|--------|------------------------|-----------|
| Logistique (OvR) | ~60% | < 1s |
| MLP [2‚Üí64‚Üí64‚Üí3] | ~97% | ~3s |
| KRR RBF (OvR) | ~95% | ~0.1s |

Convergence optimiseurs sur Rosenbrock : Adam > RMSProp > Momentum > SGD

---

## Structure math√©matique

Le rapport complet (`report/Rapport_ML_AHNANI_Ali.pdf`) couvre :

- **Autodiff :** preuve de correction du backward par tri topologique, complexit√© `O(|graph|)`
- **SGD :** th√©or√®me de convergence `O(DG/‚àöT)` pour fonctions convexes Lipschitz
- **Adam :** invariance √† l'√©chelle, justification de la correction du biais
- **BCE :** preuve de convexit√© via Hessienne `X·µÄ diag(p(1-p)) X ‚™∞ 0`
- **He init :** pr√©servation de la variance avec ReLU (`œÉ¬≤ = 2/d_in`)
- **RKHS :** th√©or√®me du repr√©sentant (Kimeldorf & Wahba 1971), d√©composition biais-variance
- **NTK :** r√©gime lazy training et connexion MLP infini ‚Üî KRR (Jacot et al. 2018)

---

## `requirements.txt`

```
numpy>=1.21
matplotlib>=3.4
```

---

## Licence

MIT ‚Äî libre d'utilisation, de modification et de distribution.

---

*AHNANI Ali ‚Äî Mini ML Framework From Scratch*
